{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for a frequency based data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the abstracts and return them as a list of sentences\n",
    "def corpus_prep(filename):\n",
    "    f = open(filename, \"r\")\n",
    "    text = f.read()\n",
    "    abstracts = text.split(\"\\n\\n\")\n",
    "    sentences = [abstract.split(\" \") for abstract in abstracts]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a frequency dictionary \n",
    "def word_count(corpus):\n",
    "    total_corpus = 0\n",
    "    counts = {}\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            total_corpus += 1\n",
    "            if word in counts:\n",
    "                counts[word] += 1\n",
    "            else:\n",
    "                counts[word] = 1\n",
    "                \n",
    "    sorted_counts = dict(sorted(counts.items(), key=lambda item: item[1], reverse=True))\n",
    "    print(total_corpus)\n",
    "    return sorted_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strips input of all punctuation and numbers\n",
    "def stripper(corpus):\n",
    "    no_punct = [[line.translate(str.maketrans('','',string.punctuation)) for line in abstract] for abstract in corpus]\n",
    "    no_nums = [[line.translate(str.maketrans('','','1234567890')) for line in abstract] for abstract in no_punct]\n",
    "    for abstr in no_nums:\n",
    "        while ('' in abstr): \n",
    "            abstr.remove('')\n",
    "    no_caps = [[sent.lower() for sent in abstr] for abstr in no_nums]\n",
    "    return no_caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters = stripper(corpus_prep(\"Data/reuters.txt\"))\n",
    "reuters_counts = word_count(reuters)\n",
    "\n",
    "jstor = stripper(corpus_prep(\"Data/jstor.txt\"))\n",
    "jstor_counts = word_count(jstor)\n",
    "\n",
    "arxiv = stripper(corpus_prep(\"Data/arxiv.txt\"))\n",
    "arxiv_counts = word_count(arxiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create intersection sets\n",
    "def intersection(x, y):\n",
    "    intersection = [name for name in x if name in y]\n",
    "    y_only = [name for name in y if name not in intersection]\n",
    "    x_only = [name for name in x if name not in intersection]\n",
    "    return x_only, intersection, y_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_jstor, jstor_reuters, only_reuters_j = intersection(jstor_counts, reuters_counts)\n",
    "only_arxiv, arxiv_reuters, only_reuters_a = intersection(arxiv_counts, reuters_counts)\n",
    "\n",
    "# Words occurring in all three corpora\n",
    "_, ajr, _ = intersection(jstor_reuters, arxiv_reuters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_jstor = len(only_jstor) + len(jstor_reuters) + len(only_reuters_j) \n",
    "total_arxiv = len(only_arxiv) + len(arxiv_reuters) + len(only_reuters_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dict = {}\n",
    "\n",
    "for key in ajr:\n",
    "    hist_dict[key] = [jstor_counts[key], arxiv_counts[key], reuters_counts[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alleen_arxiv = sum(arxiv_counts[key] for key in only_arxiv)\n",
    "alleen_reuters_a = sum(reuters_counts[key] for key in only_reuters_a)\n",
    "arx_reut = sum(arxiv_counts[key] for key in arxiv_reuters) + sum(reuters_counts[key] for key in arxiv_reuters)\n",
    "totaal_a = alleen_arxiv + alleen_reuters_a + arx_reut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alleen_jstor = sum(jstor_counts[key] for key in only_jstor)\n",
    "alleen_reuters_j = sum(reuters_counts[key] for key in only_reuters_j)\n",
    "jstor_reut = sum(jstor_counts[key] for key in jstor_reuters) + sum(reuters_counts[key] for key in jstor_reuters)\n",
    "totaal_j = alleen_jstor + alleen_reuters_j + jstor_reut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a barplot of both intersections as defined above\n",
    "\n",
    "def intersection_plot(option):\n",
    "    \n",
    "    # Set parameters\n",
    "    if option == 'total_normalised':\n",
    "        plt.ylabel(\"Number of total words (%)\")\n",
    "        plt.title(\"Total wordcount intersection\")\n",
    "\n",
    "        bars1 = [alleen_reuters_j / totaal_j, alleen_reuters_a / totaal_a]\n",
    "        bars2 = [jstor_reut / totaal_j, arx_reut / totaal_a]\n",
    "        bars3 = [alleen_jstor / totaal_j, alleen_arxiv / totaal_a]\n",
    "    \n",
    "        \n",
    "    elif option == 'unique_normalised':\n",
    "        plt.ylabel(\"Unique word occurences (%)\")\n",
    "        plt.title(\"Unique word occurence intersection\")\n",
    "    \n",
    "        bars1 = [len(only_reuters_j) / total_jstor, len(only_reuters_a) / total_arxiv]\n",
    "        bars2 = [len(jstor_reuters) / total_jstor, len(arxiv_reuters) / total_arxiv]\n",
    "        bars3 = [len(only_jstor) / total_jstor, len(only_arxiv) / total_arxiv]\n",
    "        \n",
    "    else:\n",
    "        print(\"Please specify method\")\n",
    "        \n",
    "    # Create barplot\n",
    "\n",
    "    rc('font', weight='bold')\n",
    "\n",
    "    bars = np.add(bars1, bars2).tolist()\n",
    "\n",
    "    r = [0,1]\n",
    "\n",
    "    names = ['Political Science','Physics']\n",
    "    barWidth = 1\n",
    "\n",
    "    plt.bar(r, bars3, bottom=bars, color='#28752c', edgecolor='white', width=barWidth, label='Abstracts only')\n",
    "    plt.bar(r, bars2, bottom=bars1, color='#aef007', edgecolor='white', width=barWidth, label='Intersection')\n",
    "    plt.bar(r, bars1, color='#13b01b', edgecolor='white', width=barWidth, label='Newspapers only')\n",
    "\n",
    "    plt.xticks(r, names, fontweight='bold')\n",
    "    plt.xlabel(\"Academic field\")\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(\"{}.pdf\".format(option), bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "#     Save figure\n",
    "#     fig = plt.get_figure()\n",
    "#     plt.savefig(\"output.png\")\n",
    "    \n",
    "    \n",
    "\n",
    "intersection_plot('total_normalised')\n",
    "intersection_plot('unique_normalised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top n words from each list\n",
    "def print_words(lst1, lst2, lst3, n):\n",
    "\n",
    "    dash = '-' * 80\n",
    "    \n",
    "    print(dash)\n",
    "    print('{:<30s}{:<30s}{:<30s}'.format(\"Abstract only\",\"Intersection\",\"Newspaper only\"))\n",
    "    print(dash)\n",
    "\n",
    "    for i in range(n):\n",
    "        print('{:<30s}{:<30s}{:<30s}'.format(lst1[i],lst2[i],lst3[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_words(only_jstor, jstor_reuters, only_reuters_j, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_words(only_arxiv, arxiv_reuters, only_reuters_a, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
